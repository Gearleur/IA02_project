# Le projet Gopher and Dodo ou le plus gros flop de l'Histoire

## Introduction üìÉ

Pour commencer ce titre un peu aguicheur, nous allons vous raconter un projet d'IA02 qui nous a pris plus d'une nuit sans dormir pendant plusieurs semaines et qui fut un √©chec total.

Le projet Gopher and Dodo est un projet pour l'UV d'IA02 dans lequel il fallait mettre en place une IA pour jouer aux jeux Dodo et Gopher que vous trouverez dans le dossier R√®gles. Ce sont des jeux extr√™mement simples et tr√®s faciles √† mettre en place. (Ce qui est en partie la cause de notre √©chec).

Pour essayer les differents algorithmes : aller [ici](#Utilisations)

## Mod√©lisation des jeux ‚ôüÔ∏è

Nous allons passer le fait d'expliquer les r√®gles du jeu mais nous allons nous attarder sur deux mod√©lisations des jeux. Dans chaque dossier de jeu, vous avez game et game_2. La premi√®re mod√©lisation est la plus complexe car elle devait s'adapter pour pouvoir impl√©menter AlphaZero. Elle prend en compte un syst√®me de tableau pour g√©rer les √©tats et un syst√®me qui permet d'encoder cet √©tat pour pouvoir le passer dans un r√©seau de neurones. Les parties qui diff√®rent d'une mod√©lisation de base sont : get_encoded_state et next_state_encoded. La fonction get_encoded_state permet, en ayant un √©tat de jeu donn√©, d'obtenir trois matrices de jeu avec : les coups adverses, les coups jouables et enfin les coups jou√©s par le joueur. La fonction next_state_encoded permet de passer d'un √©tat de jeu √† un autre en donnant un coup √† partir d'une matrice 1D de l'ensemble des coups jouables. Par la suite, cette mod√©lisation nous simplifiera l'utilisation d'un mod√®le ResNet pour AlphaZero.

La deuxi√®me mod√©lisation est beaucoup plus classique avec l'utilisation de dictionnaires pour g√©rer les √©tats et les coups. Cela permet de simplifier l'impl√©mentation de l'IA comme MinMax et simplifie grandement les calculs et la gestion des √©tats.

## Premi√®re exploration ü§ñ

Apr√®s avoir mod√©lis√© les jeux assez rapidement et fait une premi√®re version de l'IA, nous avons constat√© que rapidement nous arrivions √† faire une IA de MinMax. L'IA n'√©tait pas tr√®s performante mais elle arrivait √† jouer correctement, ce qui nous satisfaisait.

Ainsi, pour aller plus loin, nous avons d√©cid√© de nous atteler √† une des derni√®res d√©couvertes pour les jeux par Google, l'impl√©mentation de AlphaZero. Nous avons donc commenc√© √† lire les articles de Google et √† essayer de comprendre comment cela fonctionnait.

## AlphaZero üëæ

Pour commencer, laissez-nous vous expliquer comment AlphaZero fusionne l'intuition et la raison pour cr√©er une intelligence artificielle exceptionnelle. Il y a deux modes de pens√©e dans le raisonnement humain : un mode rapide bas√© sur l'intuition et un mode lent guid√© par des r√®gles explicites.

Dans AlphaZero, le mode rapide est repr√©sent√© par un r√©seau de neurones qui prend un √©tat de jeu et produit une politique (une distribution de probabilit√© sur les actions) et une valeur (un score indiquant la qualit√© de cet √©tat pour le joueur actuel). Le mode lent, quant √† lui, est incarn√© par une recherche d'arbre de Monte Carlo (MCTS). Imaginez que nous r√©fl√©chissons √† la prochaine action √† prendre dans un jeu d'information parfaite comme le jeu de R√©action en cha√Æne.

Nous pourrions avoir une intuition sur les meilleures actions √† prendre. Cette intuition initiale peut √™tre exprim√©e sous forme de distribution de probabilit√© sur les actions, attribuant une probabilit√© plus √©lev√©e aux bonnes actions et plus faible aux mauvaises. Cette distribution est notre "politique" pour cet √©tat donn√©. Pour am√©liorer cette politique initiale, nous pouvons envisager les mouvements futurs possibles, en utilisant notre intuition pour √©valuer les √©tats interm√©diaires et √©viter de passer trop de temps sur des n≈ìuds √† faible valeur. Apr√®s cette recherche d'arbre, nous aurons une meilleure id√©e des actions √† entreprendre, obtenant ainsi une politique am√©lior√©e. Ce processus est appel√© "amplification" et il est r√©alis√© par MCTS dans AlphaZero. Ensuite, nous utilisons cette politique am√©lior√©e pour optimiser notre r√©seau de neurones, en minimisant la perte d'entropie crois√©e entre la politique am√©lior√©e et la politique initiale, ainsi qu'une autre perte entre les pr√©dictions de valeur du r√©seau de neurones et la valeur r√©elle obtenue √† la fin d'une partie. En combinant ces deux processus, AlphaZero parvient √† d√©velopper des agents experts capables de jouer √† des jeux de mani√®re tr√®s efficace.

## MCTS 

Pour comprendre en d√©tail toutes les √©tapes de la recherche d'arbre de Monte Carlo (MCTS), nous devons commencer par une vue d'ensemble. Dans MCTS appliqu√© aux jeux, nous effectuons des simulations r√©p√©t√©es du jeu √† partir d'un √©tat de plateau donn√©. Dans la MCTS traditionnelle, ces simulations sont men√©es jusqu'√† la fin du jeu. Cependant, l'impl√©mentation de MCTS dans AlphaZero est diff√©rente de la m√©thode traditionnelle car AlphaZero utilise √©galement un r√©seau de neurones entra√Æn√© pour fournir des politiques et des valeurs pour un √©tat de plateau donn√©.

Les entr√©es de l'algorithme de recherche dans AlphaZero sont un √©tat de plateau (not√© œÉ) et le nombre d'it√©rations (√©galement appel√© le nombre de simulations) pour lesquelles nous souhaitons ex√©cuter MCTS. Dans notre cas, la sortie de cet algorithme de recherche serait la politique √† partir de laquelle nous s√©lectionnerions une action √† jouer pour l'√©tat œÉ.

L'arbre est construit de mani√®re it√©rative. Chaque n≈ìud de l'arbre contient un √©tat de plateau et des informations sur les actions valides possibles dans cet √©tat. En utilisant cette structure, AlphaZero peut am√©liorer continuellement ses d√©cisions en combinant la recherche approfondie de MCTS avec les pr√©dictions fournies par le r√©seau de neurones, ce qui conduit √† une politique de jeu optimis√©e pour chaque situation rencontr√©e.

![State](img/mcts.png)*

### S√©lection üéØ

La premi√®re √©tape de MCTS est la s√©lection. On choisit les meilleures ar√™tes √† partir du n≈ìud racine jusqu'√† atteindre un n≈ìud terminal ou un n≈ìud non explor√©. Les "meilleures ar√™tes" sont d√©termin√©es par un √©quilibre entre exploration et exploitation, guid√© par le r√©seau de neurones. L'exploration consiste √† d√©couvrir de nouvelles informations en visitant de nouveaux n≈ìuds, tandis que l'exploitation utilise les informations existantes pour choisir les n≈ìuds prometteurs.

En pratique, cette phase de s√©lection suit les ar√™tes avec les scores les plus √©lev√©s, √©quilibrant les gains attendus et le potentiel de d√©couverte. Cela garantit que l'algorithme explore suffisamment tout en exploitant les actions b√©n√©fiques, maximisant ainsi les chances de trouver une strat√©gie gagnante.

## Comprendre la r√®gle PUCT üß†

AlphaZero utilise une r√®gle appel√©e PUCT (Predictor Upper Confidence bounds applied to Trees) pour trouver un √©quilibre. Cette r√®gle a √©t√© con√ßue de mani√®re empirique, inspir√©e par les travaux de Rosin dans un cadre de bandits avec pr√©dicteurs. Un article r√©cent de DeepMind discute de quelques alternatives √† la PUCT.

La r√®gle PUCT a √©t√© d√©velopp√©e pour g√©rer les compromis entre exploration et exploitation dans les arbres de recherche. Elle utilise des pr√©dictions pour guider la recherche, permettant √† AlphaZero de naviguer efficacement dans l'espace de jeu.

Si vous voulez plus d'informations sur la r√®gle PUCT, nous vous invitons √† lire l'article suivant : [PUCT](https://medium.com/@bentou.pub/alphazero-from-scratch-in-pytorch-for-the-game-of-chain-reaction-part-2-b2e7edda14fb)

Mais voici une petite image de la formule expliquant la r√®gle PUCT et l'exploration

![PUCT](img/puct.png)*

![Exploration](img/exploration_puct.png)*

Pour bien comprendre comment fonctionne la r√®gle PUCT d'AlphaZero, prenons un exemple concret. Disons que notre r√©seau neuronal, apr√®s avoir √©t√© entra√Æn√©, nous dit avec une probabilit√© de 0,3 qu'il faut jouer une action particuli√®re, appelons-la "a". On int√®gre cette probabilit√© de 0,3 dans la partie exploration de notre r√®gle PUCT.

Imaginons maintenant que l'√©tat "s" appartient au n≈ìud parent et que l'√©tat obtenu en prenant l'action "a" sur "s" appartient au n≈ìud enfant. Si on visite un n≈ìud particulier trop souvent dans notre recherche MCTS, pour √©viter cela et encourager l'exploration d'autres n≈ìuds, on inclut le nombre de visites du n≈ìud enfant dans le d√©nominateur, et on le normalise en utilisant le nombre total de visites du n≈ìud parent.

Pourquoi prend-on la racine carr√©e du nombre de visites du n≈ìud parent ? Cette r√®gle PUCT a √©t√© con√ßue de mani√®re empirique, et c'est ce qui a donn√© les meilleurs r√©sultats parmi toutes les options test√©es par les chercheurs. En gros, on peut voir √ßa comme une mani√®re de normaliser le terme child.N + 1 dans le d√©nominateur.

Il y a un hyperparam√®tre appel√© c_puct que l'on voit dans la figure ci-dessus. Cette constante √©quilibre les termes d'exploitation et d'exploration. Une valeur typique pour cet hyperparam√®tre est de 2.

Maintenant qu'on a une id√©e de comment obtenir PUCT(s, a), revenons √† l'√©tape de s√©lection dans MCTS.

L'√©tape de s√©lection est comme montr√© dans le bloc ci-dessous : en partant de la racine, on cherche de mani√®re r√©p√©t√©e le n≈ìud enfant ayant la valeur PUCT maximale jusqu'√† ce qu'on atteigne un n≈ìud dont l'√©tat est encore None (pas encore explor√©/initialis√©).

### Expansion et √©valuation üßê

Une fois la s√©lection faite, la prochaine √©tape est d'√©tendre et d'√©valuer ce n≈ìud (dont l'√©tat est encore None). L'extension signifie qu'on initialise l'√©tat du n≈ìud s√©lectionn√© selon les r√®gles du jeu. Si le n≈ìud est terminal, on laisse l'√©tat √† None et on marque le n≈ìud comme terminal avec l'information du gagnant.

Toutes les nouvelles ar√™tes du n≈ìud s√©lectionn√© sont aussi initialis√©es. Par exemple, apr√®s l'extension, l'arbre ressemblera √† la figure ci-dessous.

Ensuite, on √©value le n≈ìud √©tendu. Par √©valuation, on cherche la r√©compense attendue pour le joueur √† ce n≈ìud. Le MCTS traditionnel effectue des simulations depuis le n≈ìud √©tendu pour trouver la valeur √† la fin du jeu, souvent de mani√®re al√©atoire.

Le MCTS d'AlphaZero est diff√©rent. Ici, on utilise la valeur de sortie du r√©seau neuronal pour d√©terminer la valeur du n≈ìud √©tendu.

C'est un peu comme √©valuer une position aux √©checs : on calcule quelques coups dans notre t√™te et on utilise notre intuition pour estimer la qualit√© de la position r√©sultante, sans faire de simulations jusqu'√† la fin du jeu avec des actions al√©atoires.

### Backup üîÑ

Apr√®s avoir √©valu√© le n≈ìud √©tendu, il faut mettre √† jour les valeurs Q (r√©alis√©es par les valeurs de r√©compense totales et les comptes de visites totales) pour tous les n≈ìuds, depuis la racine jusqu'au n≈ìud √©tendu. C'est ce qu'on appelle l'√©tape de backup dans le MCTS.

## R√©seau de neurones üß†

### Sch√©ma global du r√©seau de neurones pour AlphaZero

```plaintext
Input
  |
Start Block: Conv2d -> BatchNorm2d -> ReLU
  |
Backbone: [ResBlock] x num_resBlocks
  |
+-------------------+
|                   |
Policy Head         Value Head
|                   |
Conv2d -> BatchNorm -> Conv2d -> BatchNorm
ReLU                ReLU
Flatten             Flatten
Linear              Linear
```

### Sch√©ma global du r√©seau

```plaintext
Input
  |
Start Block: Conv2d -> BatchNorm2d -> ReLU
  |
Backbone: [ResBlock] x num_resBlocks
  |
+-------------------+
|                   |
Policy Head         Value Head
|                   |
Conv2d -> BatchNorm -> Conv2d -> BatchNorm
ReLU                ReLU
Flatten             Flatten
Linear              Linear
```


### Diagramme du r√©seau de neurones pour AlphaZero

```plaintext
Input
  |
  V
-------------------------
| Start Block           |
| Conv2d (3 -> num_hidden) |
| BatchNorm2d           |
| ReLU                  |
-------------------------
  |
  V
-------------------------
| Backbone (num_resBlocks)|
| [ResBlock] x num_resBlocks|
-------------------------
  |
  V
-------------------------------------
| Policy Head                      | Value Head
| Conv2d (num_hidden -> 32)        | Conv2d (num_hidden -> 3)
| BatchNorm2d                      | BatchNorm2d
| ReLU                             | ReLU
| Flatten                          | Flatten
| Linear -> game.action_size       | Linear -> 1
|                                  | Tanh
-------------------------------------
```


## Am√©liorations faites üöÄ

La premi√®re am√©lioration que nous avons pu faire pour l'entra√Ænement est de parall√©liser l'entra√Ænement. En effet, l'entra√Ænement d'AlphaZero est tr√®s long et peut prendre plusieurs jours. Ainsi, nous avons d√©cid√© de parall√©liser l'entra√Ænement en utilisant plusieurs processus pour entra√Æner le r√©seau de neurones. On retrouve √ßa dans AlphaZeroParallele et MCTSParallele. Cela permet de gagner un temps consid√©rable. Il a fallu √©galement traiter des listes de listes d'√©tats encod√©s, ce qui ne fut pas une mince affaire au d√©but.

## R√©sultats üìä

Sur notre ordinateur, nous n'avons pu entra√Æner notre mod√®le uniquement sur 40 parties, ce qui est clairement insuffisant pour avoir un mod√®le performant. Dans le fichier visualisation, vous pourrez voir les r√©sultats de l'entra√Ænement des mod√®les si vous d√©cidez d'essayer de les entra√Æner.

Nous pensons qu'il faut compter environ 3000 parties pour avoir un mod√®le performant. Donc il faut clairement am√©liorer le code pour l'optimiser, en particulier pour la mod√©lisation du jeu.


## Am√©lioration possible üÜô

Nous n'avons utilis√© qu'au d√©but un tableau pour voir les pions plac√©s. Mais prendre un √©tat muni d'une grille et d'un dictionnaire n'est clairement pas une mauvaise id√©e, et donc il faudrait revoir la mod√©lisation du jeu pour l'optimiser.

Une autre id√©e que nous n'avons pas eu le temps de faire est de consid√©rer uniquement un √©tat de jeu et par une rotation de 60 degr√©s, on retrouve facilement les autres √©tats de jeu. Cela permettrait de r√©duire le nombre d'√©tats de jeu et donc de r√©duire le temps d'entra√Ænement. Une des am√©liorations les plus importantes que nous n'avons malheureusement pas eu le temps de faire. (c'est, nous imaginons, ce sur quoi vous nous attendiez √©videmment...)
(C'est une partie que j'ai impl√©menter dans la partie de AlphazeroParallel. Me si je n'ai pas eu le temps de la tester.)


Enfin, comme vous avez √† disposition des IA de jeux plut√¥t tr√®s performantes, une bonne id√©e serait de commencer l'entra√Ænement du mod√®le avec ces IA pour qu'elle atteigne plus rapidement un niveau de jeu correct. Nous pensons que l'entra√Ænement de l'IA sera extr√™mement plus rapide au d√©but.


## Conclusion üéâ

Ce projet, clairement, nous avons ador√© le faire, nous avons appris √©norm√©ment de choses. Notre seul regret est de ne pas avoir pu le finir correctement. Il y a √©norm√©ment de choses que nous aurions voulu faire et c'√©tait s√ªrement trop ambitieux pour notre groupe. Nous ne nous sommes peut-√™tre pas assez rapproch√©s des professeurs pour nous aider au bon d√©roulement de ce projet. Mais nous sommes tr√®s contents de ce que nous avons pu faire et nous esp√©rons que vous avez appr√©ci√© ce projet autant que nous.

Le projet √©tait tr√®s int√©ressant. La libert√© du sujet m'a permis d'explorer et de d√©couvrir divers algorithmes, m√™me si je n'ai pas r√©ussi √† les faire tous marcher. Ces difficult√©s ont p√ª √™tre surmont√©es gr√¢ce aux capacit√©s de mon bin√¥me. Me concentrer sur des t√¢ches et algorithmes plus simples m'a permis de progresser et de faire progresser le projet. Cette exp√©rience √©tait enrichissante et formatrice.

# Utilisations {#Utilisations}

### Attention Important üö® ‚ö†Ô∏è ‚ùó

Avant d'installer les requirements, il faut installer PyTorch et CUDA en fonction de votre machine. Pour cela, je vous invite √† vous rendre sur le site de PyTorch et de suivre les instructions pour installer PyTorch en fonction de votre machine.  voir [Pytorch](https://pytorch.org/get-started/locally/)

De plus je ne vais pas transmetre les models dans le zip car trop gros mais vous pouvez les r√©cuperer sur mon [github](https://github.com/Gearleur/IA02_project)

### Cr√©ation de l'environnement virtuel
```bash
python -m venv env
```

### Activation de l'environnement virtuel
#### Pour Windows
```bash
source env/Scripts/activate
```
#### Pour Linux Mac
```bash
source env/bin/activate
```

### Installation des requierements
```bash
pip install -r requirements.txt
```

### Lancer l'entrainnement
```bash
python train_mcts.py
```

- "num_searches": nombre de recherche pour MCTS
- "C": constante pour la r√®gle PUCT
- "num_iterations": nombre d'it√©ration pour l'entrainnement
- "num_selfPlay_iterations": nombre de partie pour l'entrainnement
- "num_parallel_games": nombre de partie en parall√®le
- "num_epochs": nombre d'epoch pour l'entrainnement
- "batch_size": taille du batch : c'est le nombre de partie que l'on va jouer avant de faire un backpropagation
- "temperature": temperature pour la distribution de probabilit√©
- "dirichlet_epsilon": epsilon pour la distribution de dirichlet
- "dirichlet_alpha": alpha pour la distribution de dirichlet

Si vous voulez charger un mod√®le pour continuer l'entra√Ænement, vous pouvez d√©commenter les lignes 17 et 18 et ajouter le chemin du mod√®le √† charger et de l'optimisateur.

Enfin, num_resBlocks et num_hidden sont les hyperparam√®tres du r√©seau de neurones. Ils servent √† d√©finir le nombre de blocs r√©siduels et le nombre de neurones dans le r√©seau de neurones. Comme les jeux sont plut√¥t simples, il n'est pas n√©cessaire d'avoir un r√©seau de neurones tr√®s complexe.

### Lancer la visualisation
```bash
python visualisation.py
```

### Lancer un jeu
```bash
python main.py
```

#### PS la repr√©sentation m√©diocre au tournoi...ü§ï

Le chagment de coordon√©e n'avait pas √©t√© fait en temps et en heure et nous n'avions pas vuq ue les coordonn√©es √©taient "invers√©es" c'est a dire : r => -r  donc evidement nous avons fait enorment de coupos ill√©gaux. De plus comme nous √©tions parti sur un temps de 500s par joueur de base evidement nous avons perdu.

Une ia MinMax avec elagage et une m√©moire des √©tats visit√© a aussi √©t√© faites pour les jeux.